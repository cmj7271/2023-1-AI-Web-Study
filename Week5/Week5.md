#### **1. `tree모델` 에는 왜 `scaling` 이 필요하지 않을까?**  

----------

`tree 모델` 을 이야기하면 보통 스무고개의 비유를 많이 이야기한다.  
스무고개에서 특정 질문의 yes/no 로 대상이 무엇인지 알아가듯이,  
`tree모델` 은 하나의 `parameter`에 대해 데이터들을 분류할 수 있는 최적의 기준을 찾아간다.  
`load_breast_cancer` 를 예로 들자면, `mean-radius` 에 대해 `target`이 `1` 인지, `0` 인지를 가장 잘 구분하는 기준값을 찾는다.  
이 때, `parameter` 간의 비교가 아닌, `parameter` 내에서의 비교이기 때문에 다른 값과 형태를 맞춰주는 `scaling` 이 필요없다고 보여진다.  

---------

#### ** 2. `Decision Tree` Vs `Random Forest`

---------

`Random Forest` 는 그 이름에서도 유추할 수 있다시피, 여러 개의 `Decision Tree`를 활용하는 모델이다.  
그렇게 사용하는 이유는 무엇일까?  
어떤 학습에서 중요한 것 중 하나는 `overfitting` 즉, 테스트 데이터에 과적합하지 않게 만드는 것이다.  
`Random Forest` 의 경우 만약 최소 노드의 개수 또는 노드의 최소 데이터 수를 설정하지 않으면, 테스트 데이터를 100%로 분류할 수 있게 된다.  
하지만, 새로운 데이터가 들어왔을 때, 즉 실제 사용하고자 할 때는 오히려 정확도가 떨어질 수 있다.  
테스트 데이터와 조금만 특성이 달라도 오판단할 가능성이 높아지기 때문이다.  
이런 `overfitting` 을 방지하기 위해 사용하는 것이 `Random Forest` 모델이다.  
이 모델은 여러개의 `Decision Tree` 를 만들고, 참고하여 결과를 결정한다.  
테스트 데이터에서 랜덤으로 뽑은 N개의 데이터셋으로 N개의 `Decision Tree`를 만들면, 서로 상대적으로 다른 모델들이 만들어진다.  
이제 각각의 모델에 대해 새로운 데이터로 나온 결과를 참고하면, 상대적으로 정확도를 높일 수 있다.  
즉, 말그대로 한명의 관점보다는 여러명의 다양한 관점에서 나온 결과가 더 정확한 것이다.  

----------

#### ** 3. `scaling` 과 `fit()` 과 `transform()`

----------

* `scaling`  
서로 다른 단위의 데이터들을 통일 시켜주는 작업  
효율적인 연산, 가중치비교의 용이함 등의 이유로 한다.  
  
* `fit()`  
`StandardScaler` 에서는 데이터들의 평균과 표준편차를 구한다.
다른 Scaler에서는 `transform()` 에 필요한 값들을 구한다.  
  
* `transform()`  
`StandardScaler` 에서는 데이터를 평균 0, 표준편차 1 에 맞춘다.  
다른 Scaler에서는 데이터를 다른 방식으로 변환해준다.  
  
  
테스트할 데이터인 `X_val`에 `fit()`을 쓰면, 테스트할 데이터의 정보를 기준으로 변환하게 되어 학습한 데이터와는 다른 형태로 입력된다.  
따라서 학습시 입력한 데이터와 동일하게 맞추기 위해 `transform()`을 사용해서 학습시 입력한 데이터 기준으로 scaling 되게 해주어야 한다.